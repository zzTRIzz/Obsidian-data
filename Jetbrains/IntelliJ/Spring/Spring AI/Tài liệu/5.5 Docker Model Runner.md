

Dưới đây là tóm tắt chi tiết về cách tích hợp Spring AI với Docker Model Runner và các điểm quan trọng cần lưu ý:

---

### **1. Tích hợp Spring AI với Docker Model Runner**
- **Mục đích**: Chạy LLM (Large Language Models) cục bộ thông qua Docker.
- **Cơ chế**: Spring AI tái sử dụng `ChatClient` của OpenAI, trỏ đến Docker Model Runner thay vì OpenAI server.

#### **Cài đặt Docker Model Runner**
- **Prerequisite**: Cài Docker Desktop (Mac 4.40.0+).
- **2 cách kích hoạt**:
  1. **Option 1** (Direct):
     ```bash
     docker desktop enable model-runner --tcp 12434
     ```
     - Set `base-url: localhost:12434/engines`.
  2. **Option 2** (Testcontainers):
     ```java
     @Container
     private static final SocatContainer socat = new SocatContainer().withTarget(80, "model-runner.docker.internal");
     ```
     - Cấu hình `base-url` động qua Testcontainers.

---

### **2. Cấu hình Spring Boot**
#### **Thêm Dependency**
- **Maven**:
  ```xml
  <dependency>
      <groupId>org.springframework.ai</groupId>
      <artifactId>spring-ai-starter-model-openai</artifactId>
  </dependency>
  ```
- **Gradle**:
  ```gradle
  implementation 'org.springframework.ai:spring-ai-starter-model-openai'
  ```

#### **Cấu hình Properties**
- **File `application.properties`**:
  ```properties
  spring.ai.openai.api-key=test  # Bất kỳ giá trị nào (không cần thật)
  spring.ai.openai.base-url=http://localhost:12434/engines
  spring.ai.openai.chat.options.model=ai/gemma3:4B-F16  # Chọn model từ Docker Hub
  spring.ai.openai.embedding.enabled=false  # Tắt embedding (không hỗ trợ)
  ```

#### **Các thuộc tính quan trọng**:
- **Retry**: Cấu hình retry khi gọi API (số lần thử, backoff...).
- **Connection**: `base-url`, `api-key`.
- **Model Options**: `temperature`, `maxTokens`, `functions` (gọi hàm), v.v.

---

### **3. Function Calling (Gọi hàm từ Model)**
- **Mục đích**: Kết hợp LLM với logic nghiệp vụ (ví dụ: lấy thời tiết).
- **Ví dụ**:
  ```java
  @Bean
  @Description("Get the weather in location")
  public Function<WeatherRequest, WeatherResponse> weatherFunction() {
      return request -> {
          double temp = request.location().contains("Amsterdam") ? 20 : 25;
          return new WeatherResponse(temp, request.unit());
      };
  }
  ```
- **Sử dụng**:
  ```java
  ChatResponse response = chatClient.prompt()
      .user("What's the weather in Amsterdam?")
      .functions("weatherFunction")  // Gọi hàm qua tên bean
      .call();
  ```

---

### **4. REST Controller Mẫu**
- **Endpoint** để gọi model:
  ```java
  @RestController
  public class ChatController {
      private final OpenAiChatModel chatModel;

      @Autowired
      public ChatController(OpenAiChatModel chatModel) {
          this.chatModel = chatModel;
      }

      @GetMapping("/ai/generate")
      public Map<String, String> generate(@RequestParam String message) {
          return Map.of("generation", chatModel.call(message));
      }

      @GetMapping("/ai/generateStream")
      public Flux<ChatResponse> generateStream(@RequestParam String message) {
          return chatModel.stream(new Prompt(message));
      }
  }
  ```

---

### **5. Lưu ý Quan Trọng**
- **Model Support**: Chọn model hỗ trợ từ Docker Hub (ví dụ: `ai/gemma3:4B-F16`).
- **Runtime Options**: Ghi đè cấu hình tại runtime:
  ```java
  chatModel.call(new Prompt(
      "Generate pirate names",
      OpenAiChatOptions.builder().model("another-model").temperature(0.5).build()
  ));
  ```
- **Proxy Function Calls**: Set `spring.ai.openai.chat.options.proxy-tool-calls=true` để tự xử lý hàm.

---

### **6. Tài Nguyên Học Tập**
- [Blog Docker Model Runner](https://www.docker.com/blog/run-llms-locally-with-docker/)
- [Spring AI Documentation](https://spring.io/projects/spring-ai)
- [Code Mẫu](https://github.com/spring-projects/spring-ai) (kiểm tra test `DockerModelRunnerWithOpenAiChatModelIT.java`).

---

**Áp dụng**: Phù hợp cho ứng dụng cần chạy LLM cục bộ, kết hợp với logic nghiệp vụ tùy chỉnh, đảm bảo privacy và giảm chi phí API.